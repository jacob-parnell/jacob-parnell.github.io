---
---
@inproceedings{parnell-etal-2022-multi,
    title = "A Multi-Document Coverage Reward for {RELAX}ed Multi-Document Summarization",
    author = "Parnell, Jacob  and
      Jauregi Unanue, Inigo  and
      Piccardi, Massimo",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    pdf = {https://aclanthology.org/2022.acl-long.351.pdf},
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.351",
    doi = "10.18653/v1/2022.acl-long.351",
    pages = "5112--5128",
    abstract = "Multi-document summarization (MDS) has made significant progress in recent years, in part facilitated by the availability of new, dedicated datasets and capacious language models. However, a standing limitation of these models is that they are trained against limited references and with plain maximum-likelihood objectives. As for many other generative tasks, reinforcement learning (RL) offers the potential to improve the training of MDS models; yet, it requires a carefully-designed reward that can ensure appropriate leverage of both the reference summaries and the input documents. For this reason, in this paper we propose fine-tuning an MDS baseline with a reward that balances a reference-based metric such as ROUGE with coverage of the input documents. To implement the approach, we utilize RELAX (Grathwohl et al., 2018), a contemporary gradient estimator which is both low-variance and unbiased, and we fine-tune the baseline in a few-shot style for both stability and computational efficiency. Experimental results over the Multi-News and WCEP MDS datasets show significant improvements of up to +0.95 pp average ROUGE score and +3.17 pp METEOR score over the baseline, and competitive results with the literature. In addition, they show that the coverage of the input documents is increased, and evenly across all documents.",
}

@inproceedings{parnell-etal-2021-rewardsofsum,
    title = "{R}ewards{O}f{S}um: Exploring Reinforcement Learning Rewards for Summarisation",
    author = "Parnell, Jacob  and
      Jauregi Unanue, Inigo  and
      Piccardi, Massimo",
    editor = "Kozareva, Zornitsa  and
      Ravi, Sujith  and
      Vlachos, Andreas  and
      Agrawal, Priyanka  and
      Martins, Andr{\'e}",
    booktitle = "Proceedings of the 5th Workshop on Structured Prediction for NLP (SPNLP 2021)",
    month = aug,
    year = "2021",
    pdf = {https://aclanthology.org/2021.spnlp-1.1.pdf},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.spnlp-1.1",
    doi = "10.18653/v1/2021.spnlp-1.1",
    pages = "1--11",
    abstract = "To date, most abstractive summarisation models have relied on variants of the negative log-likelihood (NLL) as their training objective. In some cases, reinforcement learning has been added to train the models with an objective that is closer to their evaluation measures (e.g. ROUGE). However, the reward function to be used within the reinforcement learning approach can play a key role for performance and is still partially unexplored. For this reason, in this paper, we propose two reward functions for the task of abstractive summarisation: the first function, referred to as RwB-Hinge, dynamically selects the samples for the gradient update. The second function, nicknamed RISK, leverages a small pool of strong candidates to inform the reward. In the experiments, we probe the proposed approach by fine-tuning an NLL pre-trained model over nine summarisation datasets of diverse size and nature. The experimental results show a consistent improvement over the negative log-likelihood baselines.",
}

@inproceedings{jauregi-unanue-etal-2021-berttune,
    title = "{BERTT}une: Fine-Tuning Neural Machine Translation with {BERTS}core",
    author = "Jauregi Unanue, Inigo  and
      Parnell, Jacob  and
      Piccardi, Massimo",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    pdf = {https://aclanthology.org/2021.acl-short.115.pdf},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.115",
    doi = "10.18653/v1/2021.acl-short.115",
    pages = "915--924",
    abstract = "Neural machine translation models are often biased toward the limited translation references seen during training. To amend this form of overfitting, in this paper we propose fine-tuning the models with a novel training objective based on the recently-proposed BERTScore evaluation metric. BERTScore is a scoring function based on contextual embeddings that overcomes the typical limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing translations that are different from the references, yet close in the contextual embedding space, to be treated as substantially correct. To be able to use BERTScore as a training objective, we propose three approaches for generating soft predictions, allowing the network to remain completely differentiable end-to-end. Experiments carried out over four, diverse language pairs show improvements of up to 0.58 pp (3.28{\%}) in BLEU score and up to 0.76 pp (0.98{\%}) in BERTScore (F{\_}BERT) when fine-tuning a strong baseline.",
}

@inproceedings{parnell-etal-2024-sumtra,
    title = "{S}um{T}ra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization",
    author = "Parnell, Jacob  and
      Jauregi Unanue, Inigo  and
      Piccardi, Massimo",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    pdf = {https://aclanthology.org/2024.naacl-long.133.pdf},
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.133",
    pages = "2399--2415",
    abstract = "Cross-lingual summarization (XLS) generates summaries in a language different from that of the input documents (e.g., English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the predominant approach to this task is to take a performing, pretrained multilingual language model (LM) and fine-tune it for XLS on the language pairs of interest. However, the scarcity of fine-tuning samples makes this approach challenging in some cases. For this reason, in this paper we propose revisiting the summarize-and-translate pipeline, where the summarization and translation tasks are performed in a sequence. This approach allows reusing the many, publicly-available resources for monolingual summarization and translation, obtaining a very competitive zero-shot performance. In addition, the proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of few-shot fine-tuning, where available. Experiments over two contemporary and widely adopted XLS datasets (CrossSum and WikiLingua) have shown the remarkable zero-shot performance of the proposed approach, and also its strong few-shot performance compared to an equivalent multilingual LM baseline, that the proposed approach has been able to outperform in many languages with only 10{\%} of the fine-tuning samples.",
}

